{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"index.md","text":""},{"location":"fact/","title":"Facts","text":""},{"location":"inference_model/","title":"Inference model","text":""},{"location":"inference_model/#tract.inference_model-classes","title":"Classes","text":""},{"location":"inference_model/#tract.inference_model.InferenceModel","title":"<code>InferenceModel</code>","text":"<p>ONNX model are loaded as an <code>InferenceModel</code>s instead of <code>Model</code>s: many ONNX models come with partial shape and element type information, while tract's <code>Model</code> assume full shape and element type knownledge. In this case, it is generally sufficient to inform tract about the input shape and type, then let tract infer the rest of the missing shape information before converting the <code>InferenceModel</code> to a regular <code>Model</code>.</p> <pre><code># load the model as an InferenceModel\nmodel = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n# set the shape and type of its first and only input\nmodel.set_input_fact(0, \"1,3,224,224,f32\")\n\n# get ready to run the model\nmodel = model.into_optimized().into_runnable()\n</code></pre> Source code in <code>tract/inference_model.py</code> <pre><code>class InferenceModel:\n\"\"\"\n    ONNX model are loaded as an\n    `InferenceModel`s instead of `Model`s: many ONNX models come with partial shape and\n    element type information, while tract's `Model` assume full shape and element type\n    knownledge. In this case, it is generally sufficient to inform tract about the input\n    shape and type, then let tract *infer* the rest of the missing shape information\n    before converting the `InferenceModel` to a regular `Model`.\n\n    ```python\n    # load the model as an InferenceModel\n    model = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n    # set the shape and type of its first and only input\n    model.set_input_fact(0, \"1,3,224,224,f32\")\n\n    # get ready to run the model\n    model = model.into_optimized().into_runnable()\n    ```\n    \"\"\"\n    def __init__(self, ptr):\n        self.ptr = ptr\n\n    def __del__(self):\n        if self.ptr:\n            check(lib.tract_inference_model_destroy(byref(self.ptr)))\n\n    def _valid(self):\n        if self.ptr == None:\n            raise TractError(\"invalid inference model (maybe already consumed ?)\")\n\n    def into_optimized(self) -&gt; Model:\n\"\"\"\n        Run the InferenceModel through the full tract optimisation pipeline to get an\n        optimised Model.\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        check(lib.tract_inference_model_into_optimized(byref(self.ptr), byref(model)))\n        return Model(model)\n\n    def into_typed(self) -&gt; Model:\n\"\"\"\n        Convert an InferenceModel to a regular typed `Model`.\n\n        This will leave the opportunity to run more transformation on the intermediary form of the\n        model, before optimisint it all the way.\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        check(lib.tract_inference_model_into_typed(byref(self.ptr), byref(model)))\n        return Model(model)\n\n    def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_inference_model_nbio(self.ptr, byref(i), None))\n        return i.value\n\n    def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_inference_model_nbio(self.ptr, None, byref(i)))\n        return i.value\n\n    def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the `input_id`th input.\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_inference_model_input_name(self.ptr, input_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def input_fact(self, input_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `input_id`th input.\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_inference_model_input_fact(self.ptr, input_id, byref(fact)))\n        return InferenceFact(fact)\n\n    def set_input_fact(self, input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `input_id`th input.\"\"\"\n        self._valid()\n        if isinstance(fact, str):\n            fact = self.fact(fact)\n        if fact == None:\n            check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, None))\n        else:\n            check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, fact.ptr))\n\n    def set_output_names(self, names: List[str]):\n\"\"\"Change the output nodes of the model\"\"\"\n        self._valid()\n        nb = len(names)\n        names_str = []\n        names_ptr = (c_char_p * nb)()\n        for ix, n in enumerate(names):\n            names_str.append(str(n).encode(\"utf-8\"))\n            names_ptr[ix] = names_str[ix]\n        check(lib.tract_inference_model_set_output_names(self.ptr, nb, names_ptr))\n\n    def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the `output_id`th output.\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_inference_model_output_name(self.ptr, output_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def output_fact(self, output_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `output_id`th output.\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_inference_model_output_fact(self.ptr, output_id, byref(fact)))\n        return InferenceFact(fact)\n\n    def set_output_fact(self, output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `output_id`th output.\"\"\"\n        self._valid()\n        if isinstance(fact, str):\n            fact = self.fact(fact)\n        if fact == None:\n            check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, None))\n        else:\n            check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, fact.ptr))\n\n    def fact(self, spec:str) -&gt; InferenceFact:\n\"\"\"\n        Parse an fact specification as an `InferenceFact`\n\n        Typical `InferenceFact` specification is in the form \"1,224,224,3,f32\". Comma-separated\n        list of dimension, one for each axis, plus an mnemonic for the element type. f32 is \n        single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.\n        \"\"\"\n        self._valid()\n        spec = str(spec).encode(\"utf-8\")\n        fact = c_void_p();\n        check(lib.tract_inference_fact_parse(self.ptr, spec, byref(fact)))\n        return InferenceFact(fact)\n\n    def analyse(self) -&gt; None:\n\"\"\"\n        Perform shape and element type inference on the model.\n        \"\"\"\n        self._valid()\n        check(lib.tract_inference_model_analyse(self.ptr, False))\n\n    def into_analysed(self) -&gt; \"InferenceModel\":\n\"\"\"\n        Perform shape and element type inference on the model.\n        \"\"\"\n        self.analyse()\n        return self\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel-functions","title":"Functions","text":""},{"location":"inference_model/#tract.inference_model.InferenceModel.into_optimized","title":"<code>into_optimized() -&gt; Model</code>","text":"<p>Run the InferenceModel through the full tract optimisation pipeline to get an optimised Model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_optimized(self) -&gt; Model:\n\"\"\"\n    Run the InferenceModel through the full tract optimisation pipeline to get an\n    optimised Model.\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    check(lib.tract_inference_model_into_optimized(byref(self.ptr), byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.into_typed","title":"<code>into_typed() -&gt; Model</code>","text":"<p>Convert an InferenceModel to a regular typed <code>Model</code>.</p> <p>This will leave the opportunity to run more transformation on the intermediary form of the model, before optimisint it all the way.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_typed(self) -&gt; Model:\n\"\"\"\n    Convert an InferenceModel to a regular typed `Model`.\n\n    This will leave the opportunity to run more transformation on the intermediary form of the\n    model, before optimisint it all the way.\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    check(lib.tract_inference_model_into_typed(byref(self.ptr), byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_count","title":"<code>input_count() -&gt; int</code>","text":"<p>Return the number of inputs of the model</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_inference_model_nbio(self.ptr, byref(i), None))\n    return i.value\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_count","title":"<code>output_count() -&gt; int</code>","text":"<p>Return the number of outputs of the model</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_inference_model_nbio(self.ptr, None, byref(i)))\n    return i.value\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_name","title":"<code>input_name(input_id: int) -&gt; str</code>","text":"<p>Return the name of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the `input_id`th input.\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_inference_model_input_name(self.ptr, input_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_fact","title":"<code>input_fact(input_id: int) -&gt; InferenceFact</code>","text":"<p>Extract the InferenceFact of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_fact(self, input_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `input_id`th input.\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_inference_model_input_fact(self.ptr, input_id, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.set_input_fact","title":"<code>set_input_fact(input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None</code>","text":"<p>Change the InferenceFact of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def set_input_fact(self, input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `input_id`th input.\"\"\"\n    self._valid()\n    if isinstance(fact, str):\n        fact = self.fact(fact)\n    if fact == None:\n        check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, None))\n    else:\n        check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, fact.ptr))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.set_output_names","title":"<code>set_output_names(names: List[str])</code>","text":"<p>Change the output nodes of the model</p> Source code in <code>tract/inference_model.py</code> <pre><code>def set_output_names(self, names: List[str]):\n\"\"\"Change the output nodes of the model\"\"\"\n    self._valid()\n    nb = len(names)\n    names_str = []\n    names_ptr = (c_char_p * nb)()\n    for ix, n in enumerate(names):\n        names_str.append(str(n).encode(\"utf-8\"))\n        names_ptr[ix] = names_str[ix]\n    check(lib.tract_inference_model_set_output_names(self.ptr, nb, names_ptr))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_name","title":"<code>output_name(output_id: int) -&gt; str</code>","text":"<p>Return the name of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the `output_id`th output.\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_inference_model_output_name(self.ptr, output_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_fact","title":"<code>output_fact(output_id: int) -&gt; InferenceFact</code>","text":"<p>Extract the InferenceFact of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_fact(self, output_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `output_id`th output.\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_inference_model_output_fact(self.ptr, output_id, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.set_output_fact","title":"<code>set_output_fact(output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None</code>","text":"<p>Change the InferenceFact of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def set_output_fact(self, output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `output_id`th output.\"\"\"\n    self._valid()\n    if isinstance(fact, str):\n        fact = self.fact(fact)\n    if fact == None:\n        check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, None))\n    else:\n        check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, fact.ptr))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.fact","title":"<code>fact(spec: str) -&gt; InferenceFact</code>","text":"<p>Parse an fact specification as an <code>InferenceFact</code></p> <p>Typical <code>InferenceFact</code> specification is in the form \"1,224,224,3,f32\". Comma-separated list of dimension, one for each axis, plus an mnemonic for the element type. f32 is  single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def fact(self, spec:str) -&gt; InferenceFact:\n\"\"\"\n    Parse an fact specification as an `InferenceFact`\n\n    Typical `InferenceFact` specification is in the form \"1,224,224,3,f32\". Comma-separated\n    list of dimension, one for each axis, plus an mnemonic for the element type. f32 is \n    single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.\n    \"\"\"\n    self._valid()\n    spec = str(spec).encode(\"utf-8\")\n    fact = c_void_p();\n    check(lib.tract_inference_fact_parse(self.ptr, spec, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.analyse","title":"<code>analyse() -&gt; None</code>","text":"<p>Perform shape and element type inference on the model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def analyse(self) -&gt; None:\n\"\"\"\n    Perform shape and element type inference on the model.\n    \"\"\"\n    self._valid()\n    check(lib.tract_inference_model_analyse(self.ptr, False))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.into_analysed","title":"<code>into_analysed() -&gt; InferenceModel</code>","text":"<p>Perform shape and element type inference on the model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_analysed(self) -&gt; \"InferenceModel\":\n\"\"\"\n    Perform shape and element type inference on the model.\n    \"\"\"\n    self.analyse()\n    return self\n</code></pre>"},{"location":"model/","title":"Model (aka Typed Model)","text":""},{"location":"model/#tract.model-classes","title":"Classes","text":""},{"location":"model/#tract.model.Model","title":"<code>Model</code>","text":""},{"location":"model/#tract.model.Model--main-model-object","title":"Main model object","text":""},{"location":"model/#tract.model.Model--central-focus-point-of-the-model-transformation-pipeline","title":"Central focus point of the model transformation pipeline","text":"<p>The Model is the central point of tract model loading and \"model cooking\". ONNX and NNEF  serialized models are converted to Model (more or less directly) before we can do anything of value with them. Model can be dumped to NNEF (or tract-opl which is NNEF plus tract proprietary extensions).</p> <p>A Model can be <code>optimize()</code>, substituing the \"high level\" operators in tract-core operator set by the best implementation available for the current system. From there it can be transformed into a  Runnable object that we will use to run.</p>"},{"location":"model/#tract.model.Model--model-cooking","title":"Model cooking","text":"<p>But some model transformations can be peformed on the <code>Model</code> class:</p> <ul> <li>declutter (getting rid of training artefacts)</li> <li>\"pulsification\" (transforming a batch-oriented model into a streaming model)</li> <li>symbol substitution (make N or Batch a fixed number, unlocking potential optimisation later on)</li> <li>static cost evalation and dynamic profiling</li> <li>...</li> </ul> <p>In some situation, these operation are done \"on-the-fly\" when a ONNX or NNEF model is loaded, at start-up time. In other situation, when start-up time becomes an issue, it may be beneficial to \"pre-cook\" the model: apply the transformations one time, serialize the model as NNEF (with tract-opl extension if needed). At start-up this model can be significantly less expensive to \"cook\" for inference.</p>"},{"location":"model/#tract.model.Model--model-and-typedmodel","title":"Model and TypedModel","text":"<p>This class is actually a wrapper around the \"TypedModel\" in Rust codebase. The \"Typed\" bit means than all shapes and element types in all input, output and temporary values must known. There is support in tract for symbols in dimensions, with some limited computation capabilities on symbolic expression. For instance, it is relatively frequent to work with a Model where all tensors shapes start with the <code>N</code> or <code>Batch</code>.</p> Source code in <code>tract/model.py</code> <pre><code>class Model:\n\"\"\"\n    # Main model object\n\n    ## Central focus point of the model transformation pipeline\n\n    The Model is the central point of tract model loading and \"model cooking\". ONNX and NNEF \n    serialized models are converted to Model (more or less directly) before we can do anything\n    of value with them. Model can be dumped to NNEF (or tract-opl which is NNEF plus tract\n    proprietary extensions).\n\n    A Model can be `optimize()`, substituing the \"high level\" operators in tract-core operator set by\n    the best implementation available for the current system. From there it can be transformed into a \n    Runnable object that we will use to run.\n\n    ## Model cooking\n\n    But some model transformations can be peformed on the `Model` class:\n\n    * declutter (getting rid of training artefacts)\n    * \"pulsification\" (transforming a batch-oriented model into a streaming model)\n    * symbol substitution (make N or Batch a fixed number, unlocking potential optimisation later on)\n    * static cost evalation and dynamic profiling\n    * ...\n\n    In some situation, these operation are done \"on-the-fly\" when a ONNX or NNEF model is loaded,\n    at start-up time. In other situation, when start-up time becomes an issue, it may be beneficial\n    to \"pre-cook\" the model: apply the transformations one time, serialize the model as NNEF (with\n    tract-opl extension if needed). At start-up this model can be significantly less expensive to\n    \"cook\" for inference.\n\n    ## Model and TypedModel\n\n    This class is actually a wrapper around the \"TypedModel\" in Rust codebase. The \"Typed\"\n    bit means than all shapes and element types in all input, output and temporary values must\n    known. There is support in tract for symbols in dimensions, with some limited computation\n    capabilities on symbolic expression. For instance, it is relatively frequent to work with\n    a Model where all tensors shapes start with the `N` or `Batch`.\n    \"\"\"\n\n    def __init__(self, ptr):\n        self.ptr = ptr\n\n    def __del__(self):\n        if self.ptr:\n            check(lib.tract_model_destroy(byref(self.ptr)))\n\n    def _valid(self):\n        if self.ptr == None:\n            raise TractError(\"invalid model (maybe already consumed ?)\")\n\n    def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_model_nbio(self.ptr, byref(i), None))\n        return i.value\n\n    def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_model_nbio(self.ptr, None, byref(i)))\n        return i.value\n\n    def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the input_id-th input\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_model_input_name(self.ptr, input_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def input_fact(self, input_id: int) -&gt; Fact:\n\"\"\"Return the fact of the input_id-th input\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_model_input_fact(self.ptr, input_id, byref(fact)))\n        return Fact(fact)\n\n    def set_output_names(self, names: List[str]):\n\"\"\"Change the output nodes of the model\"\"\"\n        self._valid()\n        nb = len(names)\n        names_str = []\n        names_ptr = (c_char_p * nb)()\n        for ix, n in enumerate(names):\n            names_str.append(str(n).encode(\"utf-8\"))\n            names_ptr[ix] = names_str[ix]\n        check(lib.tract_model_set_output_names(self.ptr, nb, names_ptr))\n\n    def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the output_id-th output\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_model_output_name(self.ptr, output_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def output_fact(self, input_id: int) -&gt; Fact:\n\"\"\"Return the fact of the output_id-th output\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_model_output_fact(self.ptr, input_id, byref(fact)))\n        return Fact(fact)\n\n    def concretize_symbols(self, values: Dict[str, int]) -&gt; None:\n\"\"\"Substitute symbols by a value\n\n        Replace all occurencies of the symbols in the dictionary, in all the Model facts shapes.\n\n        While this is not strictly necesary, the optimizing steps may make better choices if the model\n        is informed of some specific symbol values.\n        \"\"\"\n        self._valid()\n        nb = len(values)\n        names_str = []\n        names = (c_char_p * nb)()\n        values_list = (c_int64 * nb)()\n        for ix, (k, v) in enumerate(values.items()):\n            names_str.append(str(k).encode(\"utf-8\"))\n            names[ix] = names_str[ix]\n            values_list[ix] = v\n        check(lib.tract_model_concretize_symbols(self.ptr, c_size_t(nb), names, values_list))\n\n    def pulse(self, symbol: str, pulse: Union[str, int]) -&gt; None:\n\"\"\"Pulsify a model.\n\n        `pulse` is typically a one-length dictionary mapping the time dimension symbol to a pulse len.\n        \"\"\"\n        self._valid()\n        check(lib.tract_model_pulse_simple(byref(self.ptr), symbol.encode(\"utf-8\"), str(pulse).encode(\"utf-8\")))\n\n    def declutter(self) -&gt; None:\n\"\"\"Declutter the model in place\"\"\"\n        self._valid()\n        check(lib.tract_model_declutter(self.ptr))\n\n    def optimize(self) -&gt; None:\n\"\"\"Optimize the model in place\"\"\"\n        self._valid()\n        check(lib.tract_model_optimize(self.ptr))\n\n    def into_decluttered(self) -&gt; \"Model\":\n\"\"\"Convenience method performing `declutter()` and returning the model\"\"\"\n        self.declutter();\n        return self\n\n    def into_optimized(self) -&gt; \"Model\":\n\"\"\"Convenience method performing `optimize()` and returning the model\"\"\"\n        self.optimize()\n        return self\n\n    def into_runnable(self) -&gt; Runnable:\n\"\"\"Transform the model into a Runnable model ready to be used\"\"\"\n        self._valid()\n        runnable = c_void_p()\n        check(lib.tract_model_into_runnable(byref(self.ptr), byref(runnable)))\n        return Runnable(runnable)\n\n    def property_keys(self) -&gt; List[str]:\n\"\"\"Extract the list of properties from a model\"\"\"\n        self._valid()\n        count = c_size_t()\n        check(lib.tract_model_property_count(self.ptr, byref(count)))\n        count = count.value\n        cstrings = (POINTER(c_char) * count)()\n        check(lib.tract_model_property_names(self.ptr, cstrings))\n        names = []\n        for i in range(0, count):\n            names.append(str(cast(cstrings[i], c_char_p).value, \"utf-8\"))\n            lib.tract_free_cstring(cstrings[i])\n        return names\n\n    def property(self, name: str) -&gt; Value:\n\"\"\"Query a property by name\"\"\"\n        self._valid()\n        value = c_void_p()\n        check(lib.tract_model_property(self.ptr, str(name).encode(\"utf-8\"), byref(value)))\n        return Value(value)\n\n    def profile_json(self, inputs: Union[None, List[Union[Value, numpy.ndarray]]]) -&gt; str:\n\"\"\"Profile the model on the provided input\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        input_values = []\n        input_ptrs = None\n        if inputs != None:\n            for v in inputs:\n                if isinstance(v, Value):\n                    input_values.append(v)\n                elif isinstance(v, numpy.ndarray):\n                    input_values.append(Value.from_numpy(v))\n                else:\n                    raise TractError(f\"Inputs must be of type tract.Value or numpy.Array, got {v}\")\n            input_ptrs = (c_void_p * len(inputs))()\n            for ix, v in enumerate(input_values):\n                input_ptrs[ix] = v.ptr\n        check(lib.tract_model_profile_json(self.ptr, input_ptrs, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n</code></pre>"},{"location":"model/#tract.model.Model-functions","title":"Functions","text":""},{"location":"model/#tract.model.Model.input_count","title":"<code>input_count() -&gt; int</code>","text":"<p>Return the number of inputs of the model</p> Source code in <code>tract/model.py</code> <pre><code>def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_model_nbio(self.ptr, byref(i), None))\n    return i.value\n</code></pre>"},{"location":"model/#tract.model.Model.output_count","title":"<code>output_count() -&gt; int</code>","text":"<p>Return the number of outputs of the model</p> Source code in <code>tract/model.py</code> <pre><code>def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_model_nbio(self.ptr, None, byref(i)))\n    return i.value\n</code></pre>"},{"location":"model/#tract.model.Model.input_name","title":"<code>input_name(input_id: int) -&gt; str</code>","text":"<p>Return the name of the input_id-th input</p> Source code in <code>tract/model.py</code> <pre><code>def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the input_id-th input\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_model_input_name(self.ptr, input_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"model/#tract.model.Model.input_fact","title":"<code>input_fact(input_id: int) -&gt; Fact</code>","text":"<p>Return the fact of the input_id-th input</p> Source code in <code>tract/model.py</code> <pre><code>def input_fact(self, input_id: int) -&gt; Fact:\n\"\"\"Return the fact of the input_id-th input\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_model_input_fact(self.ptr, input_id, byref(fact)))\n    return Fact(fact)\n</code></pre>"},{"location":"model/#tract.model.Model.set_output_names","title":"<code>set_output_names(names: List[str])</code>","text":"<p>Change the output nodes of the model</p> Source code in <code>tract/model.py</code> <pre><code>def set_output_names(self, names: List[str]):\n\"\"\"Change the output nodes of the model\"\"\"\n    self._valid()\n    nb = len(names)\n    names_str = []\n    names_ptr = (c_char_p * nb)()\n    for ix, n in enumerate(names):\n        names_str.append(str(n).encode(\"utf-8\"))\n        names_ptr[ix] = names_str[ix]\n    check(lib.tract_model_set_output_names(self.ptr, nb, names_ptr))\n</code></pre>"},{"location":"model/#tract.model.Model.output_name","title":"<code>output_name(output_id: int) -&gt; str</code>","text":"<p>Return the name of the output_id-th output</p> Source code in <code>tract/model.py</code> <pre><code>def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the output_id-th output\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_model_output_name(self.ptr, output_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"model/#tract.model.Model.output_fact","title":"<code>output_fact(input_id: int) -&gt; Fact</code>","text":"<p>Return the fact of the output_id-th output</p> Source code in <code>tract/model.py</code> <pre><code>def output_fact(self, input_id: int) -&gt; Fact:\n\"\"\"Return the fact of the output_id-th output\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_model_output_fact(self.ptr, input_id, byref(fact)))\n    return Fact(fact)\n</code></pre>"},{"location":"model/#tract.model.Model.concretize_symbols","title":"<code>concretize_symbols(values: Dict[str, int]) -&gt; None</code>","text":"<p>Substitute symbols by a value</p> <p>Replace all occurencies of the symbols in the dictionary, in all the Model facts shapes.</p> <p>While this is not strictly necesary, the optimizing steps may make better choices if the model is informed of some specific symbol values.</p> Source code in <code>tract/model.py</code> <pre><code>def concretize_symbols(self, values: Dict[str, int]) -&gt; None:\n\"\"\"Substitute symbols by a value\n\n    Replace all occurencies of the symbols in the dictionary, in all the Model facts shapes.\n\n    While this is not strictly necesary, the optimizing steps may make better choices if the model\n    is informed of some specific symbol values.\n    \"\"\"\n    self._valid()\n    nb = len(values)\n    names_str = []\n    names = (c_char_p * nb)()\n    values_list = (c_int64 * nb)()\n    for ix, (k, v) in enumerate(values.items()):\n        names_str.append(str(k).encode(\"utf-8\"))\n        names[ix] = names_str[ix]\n        values_list[ix] = v\n    check(lib.tract_model_concretize_symbols(self.ptr, c_size_t(nb), names, values_list))\n</code></pre>"},{"location":"model/#tract.model.Model.pulse","title":"<code>pulse(symbol: str, pulse: Union[str, int]) -&gt; None</code>","text":"<p>Pulsify a model.</p> <p><code>pulse</code> is typically a one-length dictionary mapping the time dimension symbol to a pulse len.</p> Source code in <code>tract/model.py</code> <pre><code>def pulse(self, symbol: str, pulse: Union[str, int]) -&gt; None:\n\"\"\"Pulsify a model.\n\n    `pulse` is typically a one-length dictionary mapping the time dimension symbol to a pulse len.\n    \"\"\"\n    self._valid()\n    check(lib.tract_model_pulse_simple(byref(self.ptr), symbol.encode(\"utf-8\"), str(pulse).encode(\"utf-8\")))\n</code></pre>"},{"location":"model/#tract.model.Model.declutter","title":"<code>declutter() -&gt; None</code>","text":"<p>Declutter the model in place</p> Source code in <code>tract/model.py</code> <pre><code>def declutter(self) -&gt; None:\n\"\"\"Declutter the model in place\"\"\"\n    self._valid()\n    check(lib.tract_model_declutter(self.ptr))\n</code></pre>"},{"location":"model/#tract.model.Model.optimize","title":"<code>optimize() -&gt; None</code>","text":"<p>Optimize the model in place</p> Source code in <code>tract/model.py</code> <pre><code>def optimize(self) -&gt; None:\n\"\"\"Optimize the model in place\"\"\"\n    self._valid()\n    check(lib.tract_model_optimize(self.ptr))\n</code></pre>"},{"location":"model/#tract.model.Model.into_decluttered","title":"<code>into_decluttered() -&gt; Model</code>","text":"<p>Convenience method performing <code>declutter()</code> and returning the model</p> Source code in <code>tract/model.py</code> <pre><code>def into_decluttered(self) -&gt; \"Model\":\n\"\"\"Convenience method performing `declutter()` and returning the model\"\"\"\n    self.declutter();\n    return self\n</code></pre>"},{"location":"model/#tract.model.Model.into_optimized","title":"<code>into_optimized() -&gt; Model</code>","text":"<p>Convenience method performing <code>optimize()</code> and returning the model</p> Source code in <code>tract/model.py</code> <pre><code>def into_optimized(self) -&gt; \"Model\":\n\"\"\"Convenience method performing `optimize()` and returning the model\"\"\"\n    self.optimize()\n    return self\n</code></pre>"},{"location":"model/#tract.model.Model.into_runnable","title":"<code>into_runnable() -&gt; Runnable</code>","text":"<p>Transform the model into a Runnable model ready to be used</p> Source code in <code>tract/model.py</code> <pre><code>def into_runnable(self) -&gt; Runnable:\n\"\"\"Transform the model into a Runnable model ready to be used\"\"\"\n    self._valid()\n    runnable = c_void_p()\n    check(lib.tract_model_into_runnable(byref(self.ptr), byref(runnable)))\n    return Runnable(runnable)\n</code></pre>"},{"location":"model/#tract.model.Model.property_keys","title":"<code>property_keys() -&gt; List[str]</code>","text":"<p>Extract the list of properties from a model</p> Source code in <code>tract/model.py</code> <pre><code>def property_keys(self) -&gt; List[str]:\n\"\"\"Extract the list of properties from a model\"\"\"\n    self._valid()\n    count = c_size_t()\n    check(lib.tract_model_property_count(self.ptr, byref(count)))\n    count = count.value\n    cstrings = (POINTER(c_char) * count)()\n    check(lib.tract_model_property_names(self.ptr, cstrings))\n    names = []\n    for i in range(0, count):\n        names.append(str(cast(cstrings[i], c_char_p).value, \"utf-8\"))\n        lib.tract_free_cstring(cstrings[i])\n    return names\n</code></pre>"},{"location":"model/#tract.model.Model.property","title":"<code>property(name: str) -&gt; Value</code>","text":"<p>Query a property by name</p> Source code in <code>tract/model.py</code> <pre><code>def property(self, name: str) -&gt; Value:\n\"\"\"Query a property by name\"\"\"\n    self._valid()\n    value = c_void_p()\n    check(lib.tract_model_property(self.ptr, str(name).encode(\"utf-8\"), byref(value)))\n    return Value(value)\n</code></pre>"},{"location":"model/#tract.model.Model.profile_json","title":"<code>profile_json(inputs: Union[None, List[Union[Value, numpy.ndarray]]]) -&gt; str</code>","text":"<p>Profile the model on the provided input</p> Source code in <code>tract/model.py</code> <pre><code>def profile_json(self, inputs: Union[None, List[Union[Value, numpy.ndarray]]]) -&gt; str:\n\"\"\"Profile the model on the provided input\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    input_values = []\n    input_ptrs = None\n    if inputs != None:\n        for v in inputs:\n            if isinstance(v, Value):\n                input_values.append(v)\n            elif isinstance(v, numpy.ndarray):\n                input_values.append(Value.from_numpy(v))\n            else:\n                raise TractError(f\"Inputs must be of type tract.Value or numpy.Array, got {v}\")\n        input_ptrs = (c_void_p * len(inputs))()\n        for ix, v in enumerate(input_values):\n            input_ptrs[ix] = v.ptr\n    check(lib.tract_model_profile_json(self.ptr, input_ptrs, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"nnef/","title":"NNEF","text":""},{"location":"nnef/#tract.nnef-classes","title":"Classes","text":""},{"location":"nnef/#tract.nnef.Nnef","title":"<code>Nnef</code>","text":"<p>Represent a NNEF context in tract.</p> <p>NNEF is a neural model interchange format, similar to ONNX but focusing on the needs of an inference engine instead of a training framework.</p> <p><code>tract</code> can natively load NNEF models. It can also save models it tract internal format as <code>tract-opl</code> models. <code>tract-opl</code> is a set of proprierary extensions to NNEF allowing to serializeing most of the models tract can handle. These extension can be activated by the <code>with_*() methods</code>.</p> Source code in <code>tract/nnef.py</code> <pre><code>class Nnef:\n\"\"\"\n    Represent a NNEF context in tract.\n\n    NNEF is a neural model interchange format, similar to ONNX but focusing on the needs\n    of an inference engine instead of a training framework.\n\n    `tract` can natively load NNEF models. It can also save models it tract internal format\n    as `tract-opl` models. `tract-opl` is a set of proprierary extensions to NNEF allowing to\n    serializeing most of the models tract can handle. These extension can be activated by the\n    `with_*() methods`.\n    \"\"\"\n\n    def __init__(self):\n        ptr = c_void_p()\n        check(lib.tract_nnef_create(byref(ptr)))\n        self.ptr = ptr\n\n    def __del__(self):\n        check(lib.tract_nnef_destroy(byref(self.ptr)))\n\n    def _valid(self):\n        if self.ptr == None:\n            raise TractError(\"invalid inference model (maybe already consumed ?)\")\n\n    def model_for_path(self, path: Union[str, Path]) -&gt; Model:\n\"\"\"\n        Load an NNEF model from the file or folder at `path`\n\n        ```python\n        model = (\n            tract.nnef()\n            .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n            .into_optimized()\n            .into_runnable()\n        )\n        ```\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_model_for_path(self.ptr, path, byref(model)))\n        return Model(model)\n\n    def with_tract_core(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF to covers tract-core operator set\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_tract_core(self.ptr))\n        return self\n\n    def with_onnx(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_onnx(self.ptr))\n        return self\n\n    def with_pulse(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_pulse(self.ptr))\n        return self\n\n    def with_extended_identifier_syntax(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF for extended identifiers (will support PyTorch 2 path-like ids)\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_allow_extended_identifier_syntax(self.ptr, True))\n        return self\n\n    def write_model_to_dir(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF directory model in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_dir(self.ptr, path, model.ptr))\n\n    def write_model_to_tar(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF tar archive in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_tar(self.ptr, path, model.ptr))\n\n    def write_model_to_tar_gz(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF tar compressed archive in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_tar_gz(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef-functions","title":"Functions","text":""},{"location":"nnef/#tract.nnef.Nnef.model_for_path","title":"<code>model_for_path(path: Union[str, Path]) -&gt; Model</code>","text":"<p>Load an NNEF model from the file or folder at <code>path</code></p> <pre><code>model = (\n    tract.nnef()\n    .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n    .into_optimized()\n    .into_runnable()\n)\n</code></pre> Source code in <code>tract/nnef.py</code> <pre><code>def model_for_path(self, path: Union[str, Path]) -&gt; Model:\n\"\"\"\n    Load an NNEF model from the file or folder at `path`\n\n    ```python\n    model = (\n        tract.nnef()\n        .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n        .into_optimized()\n        .into_runnable()\n    )\n    ```\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_model_for_path(self.ptr, path, byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_tract_core","title":"<code>with_tract_core() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF to covers tract-core operator set</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_tract_core(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF to covers tract-core operator set\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_tract_core(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_onnx","title":"<code>with_onnx() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_onnx(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_onnx(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_pulse","title":"<code>with_pulse() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_pulse(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_pulse(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_extended_identifier_syntax","title":"<code>with_extended_identifier_syntax() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF for extended identifiers (will support PyTorch 2 path-like ids)</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_extended_identifier_syntax(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF for extended identifiers (will support PyTorch 2 path-like ids)\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_allow_extended_identifier_syntax(self.ptr, True))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_dir","title":"<code>write_model_to_dir(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF directory model in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_dir(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF directory model in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_dir(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_tar","title":"<code>write_model_to_tar(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF tar archive in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_tar(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF tar archive in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_tar(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_tar_gz","title":"<code>write_model_to_tar_gz(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF tar compressed archive in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_tar_gz(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF tar compressed archive in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_tar_gz(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"onnx/","title":"ONNX","text":""},{"location":"onnx/#tract.onnx-classes","title":"Classes","text":""},{"location":"onnx/#tract.onnx.Onnx","title":"<code>Onnx</code>","text":"<p>Represent the ONNX context in tract.</p> <p>It essentially allows to load ONNX models. Note that an ONNX model is loaded as an <code>InferenceModel</code> and not as a <code>Model</code>: many ONNX models come with partial shape and element type information, while tract's <code>Model</code> assume full shape and element type knownledge. In this case, it is generally sufficient to inform tract about the input shape and type, then let tract infer the rest of the missing shape information before converting the <code>InferenceModel</code> to a regular <code>Model</code>.</p> <pre><code># load the model as an InferenceModel\nmodel = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n# set the shape and type of its first and only input\nmodel.set_input_fact(0, \"1,3,224,224,f32\")\n\n# get ready to run the model\nmodel = model.into_optimized().into_runnable()\n</code></pre> Source code in <code>tract/onnx.py</code> <pre><code>class Onnx:\n\"\"\"\n    Represent the ONNX context in tract.\n\n    It essentially allows to load ONNX models. Note that an ONNX model is loaded as an\n    `InferenceModel` and not as a `Model`: many ONNX models come with partial shape and\n    element type information, while tract's `Model` assume full shape and element type\n    knownledge. In this case, it is generally sufficient to inform tract about the input\n    shape and type, then let tract *infer* the rest of the missing shape information\n    before converting the `InferenceModel` to a regular `Model`.\n\n    ```python\n    # load the model as an InferenceModel\n    model = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n    # set the shape and type of its first and only input\n    model.set_input_fact(0, \"1,3,224,224,f32\")\n\n    # get ready to run the model\n    model = model.into_optimized().into_runnable()\n    ```\n    \"\"\"\n\n    def __init__(self):\n        ptr = c_void_p()\n        check(lib.tract_onnx_create(byref(ptr)))\n        self.ptr = ptr\n\n    def __del__(self):\n        check(lib.tract_onnx_destroy(byref(self.ptr)))\n\n    def model_for_path(self, path: Union[str, Path]) -&gt; InferenceModel:\n\"\"\"\n        Load an ONNX file as an InferenceModel\n        \"\"\"\n        model = c_void_p()\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_onnx_model_for_path(self.ptr, path, byref(model)))\n        return InferenceModel(model)\n</code></pre>"},{"location":"onnx/#tract.onnx.Onnx-functions","title":"Functions","text":""},{"location":"onnx/#tract.onnx.Onnx.model_for_path","title":"<code>model_for_path(path: Union[str, Path]) -&gt; InferenceModel</code>","text":"<p>Load an ONNX file as an InferenceModel</p> Source code in <code>tract/onnx.py</code> <pre><code>def model_for_path(self, path: Union[str, Path]) -&gt; InferenceModel:\n\"\"\"\n    Load an ONNX file as an InferenceModel\n    \"\"\"\n    model = c_void_p()\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_onnx_model_for_path(self.ptr, path, byref(model)))\n    return InferenceModel(model)\n</code></pre>"},{"location":"runnable/","title":"Runnable model","text":""},{"location":"value/","title":"Value","text":""}]}