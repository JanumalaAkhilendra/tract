{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"index.md","text":""},{"location":"fact/","title":"Facts","text":""},{"location":"inference_model/","title":"Inference model","text":""},{"location":"inference_model/#tract.inference_model-classes","title":"Classes","text":""},{"location":"inference_model/#tract.inference_model.InferenceModel","title":"<code>InferenceModel</code>","text":"<p>ONNX model are loaded as an <code>InferenceModel</code>s instead of <code>Model</code>s: many ONNX models come with partial shape and element type information, while tract's <code>Model</code> assume full shape and element type knownledge. In this case, it is generally sufficient to inform tract about the input shape and type, then let tract infer the rest of the missing shape information before converting the <code>InferenceModel</code> to a regular <code>Model</code>.</p> <pre><code># load the model as an InferenceModel\nmodel = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n# set the shape and type of its first and only input\nmodel.set_input_fact(0, \"1,3,224,224,f32\")\n\n# get ready to run the model\nmodel = model.into_optimized().into_runnable()\n</code></pre> Source code in <code>tract/inference_model.py</code> <pre><code>class InferenceModel:\n\"\"\"\n    ONNX model are loaded as an\n    `InferenceModel`s instead of `Model`s: many ONNX models come with partial shape and\n    element type information, while tract's `Model` assume full shape and element type\n    knownledge. In this case, it is generally sufficient to inform tract about the input\n    shape and type, then let tract *infer* the rest of the missing shape information\n    before converting the `InferenceModel` to a regular `Model`.\n\n    ```python\n    # load the model as an InferenceModel\n    model = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n    # set the shape and type of its first and only input\n    model.set_input_fact(0, \"1,3,224,224,f32\")\n\n    # get ready to run the model\n    model = model.into_optimized().into_runnable()\n    ```\n    \"\"\"\n    def __init__(self, ptr):\n        self.ptr = ptr\n\n    def __del__(self):\n        if self.ptr:\n            check(lib.tract_inference_model_destroy(byref(self.ptr)))\n\n    def _valid(self):\n        if self.ptr == None:\n            raise TractError(\"invalid inference model (maybe already consumed ?)\")\n\n    def into_optimized(self) -&gt; Model:\n\"\"\"\n        Run the InferenceModel through the full tract optimisation pipeline to get an\n        optimised Model.\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        check(lib.tract_inference_model_into_optimized(byref(self.ptr), byref(model)))\n        return Model(model)\n\n    def into_typed(self) -&gt; Model:\n\"\"\"\n        Convert an InferenceModel to a regular typed `Model`.\n\n        This will leave the opportunity to run more transformation on the intermediary form of the\n        model, before optimisint it all the way.\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        check(lib.tract_inference_model_into_typed(byref(self.ptr), byref(model)))\n        return Model(model)\n\n    def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_inference_model_nbio(self.ptr, byref(i), None))\n        return i.value\n\n    def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n        self._valid()\n        i = c_size_t()\n        check(lib.tract_inference_model_nbio(self.ptr, None, byref(i)))\n        return i.value\n\n    def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the `input_id`th input.\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_inference_model_input_name(self.ptr, input_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def input_fact(self, input_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `input_id`th input.\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_inference_model_input_fact(self.ptr, input_id, byref(fact)))\n        return InferenceFact(fact)\n\n    def set_input_fact(self, input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `input_id`th input.\"\"\"\n        self._valid()\n        if isinstance(fact, str):\n            fact = self.fact(fact)\n        if fact == None:\n            check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, None))\n        else:\n            check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, fact.ptr))\n\n    def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the `output_id`th output.\"\"\"\n        self._valid()\n        cstring = c_char_p()\n        check(lib.tract_inference_model_output_name(self.ptr, output_id, byref(cstring)))\n        result = str(cstring.value, \"utf-8\")\n        lib.tract_free_cstring(cstring)\n        return result\n\n    def output_fact(self, output_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `output_id`th output.\"\"\"\n        self._valid()\n        fact = c_void_p()\n        check(lib.tract_inference_model_output_fact(self.ptr, output_id, byref(fact)))\n        return InferenceFact(fact)\n\n    def set_output_fact(self, output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `output_id`th output.\"\"\"\n        self._valid()\n        if isinstance(fact, str):\n            fact = self.fact(fact)\n        if fact == None:\n            check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, None))\n        else:\n            check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, fact.ptr))\n\n    def fact(self, spec:str) -&gt; InferenceFact:\n\"\"\"\n        Parse an fact specification as an `InferenceFact`\n\n        Typical `InferenceFact` specification is in the form \"1,224,224,3,f32\". Comma-separated\n        list of dimension, one for each axis, plus an mnemonic for the element type. f32 is \n        single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.\n        \"\"\"\n        self._valid()\n        spec = str(spec).encode(\"utf-8\")\n        fact = c_void_p();\n        check(lib.tract_inference_fact_parse(self.ptr, spec, byref(fact)))\n        return InferenceFact(fact)\n\n    def analyse(self) -&gt; None:\n\"\"\"\n        Perform shape and element type inference on the model.\n        \"\"\"\n        self._valid()\n        check(lib.tract_inference_model_analyse(self.ptr, False))\n\n    def into_analysed(self) -&gt; \"InferenceModel\":\n\"\"\"\n        Perform shape and element type inference on the model.\n        \"\"\"\n        self.analyse()\n        return self\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel-functions","title":"Functions","text":""},{"location":"inference_model/#tract.inference_model.InferenceModel.into_optimized","title":"<code>into_optimized() -&gt; Model</code>","text":"<p>Run the InferenceModel through the full tract optimisation pipeline to get an optimised Model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_optimized(self) -&gt; Model:\n\"\"\"\n    Run the InferenceModel through the full tract optimisation pipeline to get an\n    optimised Model.\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    check(lib.tract_inference_model_into_optimized(byref(self.ptr), byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.into_typed","title":"<code>into_typed() -&gt; Model</code>","text":"<p>Convert an InferenceModel to a regular typed <code>Model</code>.</p> <p>This will leave the opportunity to run more transformation on the intermediary form of the model, before optimisint it all the way.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_typed(self) -&gt; Model:\n\"\"\"\n    Convert an InferenceModel to a regular typed `Model`.\n\n    This will leave the opportunity to run more transformation on the intermediary form of the\n    model, before optimisint it all the way.\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    check(lib.tract_inference_model_into_typed(byref(self.ptr), byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_count","title":"<code>input_count() -&gt; int</code>","text":"<p>Return the number of inputs of the model</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_count(self) -&gt; int:\n\"\"\"Return the number of inputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_inference_model_nbio(self.ptr, byref(i), None))\n    return i.value\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_count","title":"<code>output_count() -&gt; int</code>","text":"<p>Return the number of outputs of the model</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_count(self) -&gt; int:\n\"\"\"Return the number of outputs of the model\"\"\"\n    self._valid()\n    i = c_size_t()\n    check(lib.tract_inference_model_nbio(self.ptr, None, byref(i)))\n    return i.value\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_name","title":"<code>input_name(input_id: int) -&gt; str</code>","text":"<p>Return the name of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_name(self, input_id: int) -&gt; str:\n\"\"\"Return the name of the `input_id`th input.\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_inference_model_input_name(self.ptr, input_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.input_fact","title":"<code>input_fact(input_id: int) -&gt; InferenceFact</code>","text":"<p>Extract the InferenceFact of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def input_fact(self, input_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `input_id`th input.\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_inference_model_input_fact(self.ptr, input_id, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.set_input_fact","title":"<code>set_input_fact(input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None</code>","text":"<p>Change the InferenceFact of the <code>input_id</code>th input.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def set_input_fact(self, input_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `input_id`th input.\"\"\"\n    self._valid()\n    if isinstance(fact, str):\n        fact = self.fact(fact)\n    if fact == None:\n        check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, None))\n    else:\n        check(lib.tract_inference_model_set_input_fact(self.ptr, input_id, fact.ptr))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_name","title":"<code>output_name(output_id: int) -&gt; str</code>","text":"<p>Return the name of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_name(self, output_id: int) -&gt; str:\n\"\"\"Return the name of the `output_id`th output.\"\"\"\n    self._valid()\n    cstring = c_char_p()\n    check(lib.tract_inference_model_output_name(self.ptr, output_id, byref(cstring)))\n    result = str(cstring.value, \"utf-8\")\n    lib.tract_free_cstring(cstring)\n    return result\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.output_fact","title":"<code>output_fact(output_id: int) -&gt; InferenceFact</code>","text":"<p>Extract the InferenceFact of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def output_fact(self, output_id: int) -&gt; InferenceFact:\n\"\"\"Extract the InferenceFact of the `output_id`th output.\"\"\"\n    self._valid()\n    fact = c_void_p()\n    check(lib.tract_inference_model_output_fact(self.ptr, output_id, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.set_output_fact","title":"<code>set_output_fact(output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None</code>","text":"<p>Change the InferenceFact of the <code>output_id</code>th output.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def set_output_fact(self, output_id: int, fact: Union[InferenceFact, str, None]) -&gt; None:\n\"\"\"Change the InferenceFact of the `output_id`th output.\"\"\"\n    self._valid()\n    if isinstance(fact, str):\n        fact = self.fact(fact)\n    if fact == None:\n        check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, None))\n    else:\n        check(lib.tract_inference_model_set_output_fact(self.ptr, output_id, fact.ptr))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.fact","title":"<code>fact(spec: str) -&gt; InferenceFact</code>","text":"<p>Parse an fact specification as an <code>InferenceFact</code></p> <p>Typical <code>InferenceFact</code> specification is in the form \"1,224,224,3,f32\". Comma-separated list of dimension, one for each axis, plus an mnemonic for the element type. f32 is  single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def fact(self, spec:str) -&gt; InferenceFact:\n\"\"\"\n    Parse an fact specification as an `InferenceFact`\n\n    Typical `InferenceFact` specification is in the form \"1,224,224,3,f32\". Comma-separated\n    list of dimension, one for each axis, plus an mnemonic for the element type. f32 is \n    single precision \"float\", i16 is a 16-bit signed integer, and u8 a 8-bit unsigned integer.\n    \"\"\"\n    self._valid()\n    spec = str(spec).encode(\"utf-8\")\n    fact = c_void_p();\n    check(lib.tract_inference_fact_parse(self.ptr, spec, byref(fact)))\n    return InferenceFact(fact)\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.analyse","title":"<code>analyse() -&gt; None</code>","text":"<p>Perform shape and element type inference on the model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def analyse(self) -&gt; None:\n\"\"\"\n    Perform shape and element type inference on the model.\n    \"\"\"\n    self._valid()\n    check(lib.tract_inference_model_analyse(self.ptr, False))\n</code></pre>"},{"location":"inference_model/#tract.inference_model.InferenceModel.into_analysed","title":"<code>into_analysed() -&gt; InferenceModel</code>","text":"<p>Perform shape and element type inference on the model.</p> Source code in <code>tract/inference_model.py</code> <pre><code>def into_analysed(self) -&gt; \"InferenceModel\":\n\"\"\"\n    Perform shape and element type inference on the model.\n    \"\"\"\n    self.analyse()\n    return self\n</code></pre>"},{"location":"model/","title":"Model (aka Typed Model)","text":""},{"location":"nnef/","title":"NNEF","text":""},{"location":"nnef/#tract.nnef-classes","title":"Classes","text":""},{"location":"nnef/#tract.nnef.Nnef","title":"<code>Nnef</code>","text":"<p>Represent a NNEF context in tract.</p> <p>NNEF is a neural model interchange format, similar to ONNX but focusing on the needs of an inference engine instead of a training framework.</p> <p><code>tract</code> can natively load NNEF models. It can also save models it tract internal format as <code>tract-opl</code> models. <code>tract-opl</code> is a set of proprierary extensions to NNEF allowing to serializeing most of the models tract can handle. These extension can be activated by the <code>with_*() methods</code>.</p> Source code in <code>tract/nnef.py</code> <pre><code>class Nnef:\n\"\"\"\n    Represent a NNEF context in tract.\n\n    NNEF is a neural model interchange format, similar to ONNX but focusing on the needs\n    of an inference engine instead of a training framework.\n\n    `tract` can natively load NNEF models. It can also save models it tract internal format\n    as `tract-opl` models. `tract-opl` is a set of proprierary extensions to NNEF allowing to\n    serializeing most of the models tract can handle. These extension can be activated by the\n    `with_*() methods`.\n    \"\"\"\n\n    def __init__(self):\n        ptr = c_void_p()\n        check(lib.tract_nnef_create(byref(ptr)))\n        self.ptr = ptr\n\n    def __del__(self):\n        check(lib.tract_nnef_destroy(byref(self.ptr)))\n\n    def _valid(self):\n        if self.ptr == None:\n            raise TractError(\"invalid inference model (maybe already consumed ?)\")\n\n    def model_for_path(self, path: Union[str, Path]) -&gt; Model:\n\"\"\"\n        Load an NNEF model from the file or folder at `path`\n\n        ```python\n        model = (\n            tract.nnef()\n            .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n            .into_optimized()\n            .into_runnable()\n        )\n        ```\n        \"\"\"\n        self._valid()\n        model = c_void_p()\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_model_for_path(self.ptr, path, byref(model)))\n        return Model(model)\n\n    def with_tract_core(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF to covers tract-core operator set\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_tract_core(self.ptr))\n        return self\n\n    def with_onnx(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_onnx(self.ptr))\n        return self\n\n    def with_pulse(self) -&gt; \"Nnef\":\n\"\"\"\n        Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)\n        \"\"\"\n        self._valid()\n        check(lib.tract_nnef_enable_pulse(self.ptr))\n        return self\n\n    def write_model_to_dir(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF directory model in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_dir(self.ptr, path, model.ptr))\n\n    def write_model_to_tar(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF tar archive in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_tar(self.ptr, path, model.ptr))\n\n    def write_model_to_tar_gz(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n        Save `model` as a NNEF tar compressed archive in `path`.\n\n        tract tries to stick to strict NNEF even if extensions has been enabled.\n        \"\"\"\n        self._valid()\n        model._valid()\n        if not isinstance(model, Model):\n            raise TractError(\"Expected a Model, called with \" + model);\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_nnef_write_model_to_tar_gz(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef-functions","title":"Functions","text":""},{"location":"nnef/#tract.nnef.Nnef.model_for_path","title":"<code>model_for_path(path: Union[str, Path]) -&gt; Model</code>","text":"<p>Load an NNEF model from the file or folder at <code>path</code></p> <pre><code>model = (\n    tract.nnef()\n    .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n    .into_optimized()\n    .into_runnable()\n)\n</code></pre> Source code in <code>tract/nnef.py</code> <pre><code>def model_for_path(self, path: Union[str, Path]) -&gt; Model:\n\"\"\"\n    Load an NNEF model from the file or folder at `path`\n\n    ```python\n    model = (\n        tract.nnef()\n        .model_for_path(\"mobilenet_v2_1.0.onnx.nnef.tgz\")\n        .into_optimized()\n        .into_runnable()\n    )\n    ```\n    \"\"\"\n    self._valid()\n    model = c_void_p()\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_model_for_path(self.ptr, path, byref(model)))\n    return Model(model)\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_tract_core","title":"<code>with_tract_core() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF to covers tract-core operator set</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_tract_core(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF to covers tract-core operator set\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_tract_core(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_onnx","title":"<code>with_onnx() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_onnx(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF to covers (more or) ONNX operator set\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_onnx(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.with_pulse","title":"<code>with_pulse() -&gt; Nnef</code>","text":"<p>Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)</p> Source code in <code>tract/nnef.py</code> <pre><code>def with_pulse(self) -&gt; \"Nnef\":\n\"\"\"\n    Enable tract-opl extensions to NNEF for tract pulse operators (for audio streaming)\n    \"\"\"\n    self._valid()\n    check(lib.tract_nnef_enable_pulse(self.ptr))\n    return self\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_dir","title":"<code>write_model_to_dir(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF directory model in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_dir(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF directory model in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_dir(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_tar","title":"<code>write_model_to_tar(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF tar archive in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_tar(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF tar archive in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_tar(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"nnef/#tract.nnef.Nnef.write_model_to_tar_gz","title":"<code>write_model_to_tar_gz(model: Model, path: Union[str, Path]) -&gt; None</code>","text":"<p>Save <code>model</code> as a NNEF tar compressed archive in <code>path</code>.</p> <p>tract tries to stick to strict NNEF even if extensions has been enabled.</p> Source code in <code>tract/nnef.py</code> <pre><code>def write_model_to_tar_gz(self, model: Model, path: Union[str, Path]) -&gt; None:\n\"\"\"\n    Save `model` as a NNEF tar compressed archive in `path`.\n\n    tract tries to stick to strict NNEF even if extensions has been enabled.\n    \"\"\"\n    self._valid()\n    model._valid()\n    if not isinstance(model, Model):\n        raise TractError(\"Expected a Model, called with \" + model);\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_nnef_write_model_to_tar_gz(self.ptr, path, model.ptr))\n</code></pre>"},{"location":"onnx/","title":"ONNX","text":""},{"location":"onnx/#tract.onnx-classes","title":"Classes","text":""},{"location":"onnx/#tract.onnx.Onnx","title":"<code>Onnx</code>","text":"<p>Represent the ONNX context in tract.</p> <p>It essentially allows to load ONNX models. Note that an ONNX model is loaded as an <code>InferenceModel</code> and not as a <code>Model</code>: many ONNX models come with partial shape and element type information, while tract's <code>Model</code> assume full shape and element type knownledge. In this case, it is generally sufficient to inform tract about the input shape and type, then let tract infer the rest of the missing shape information before converting the <code>InferenceModel</code> to a regular <code>Model</code>.</p> <pre><code># load the model as an InferenceModel\nmodel = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n# set the shape and type of its first and only input\nmodel.set_input_fact(0, \"1,3,224,224,f32\")\n\n# get ready to run the model\nmodel = model.into_optimized().into_runnable()\n</code></pre> Source code in <code>tract/onnx.py</code> <pre><code>class Onnx:\n\"\"\"\n    Represent the ONNX context in tract.\n\n    It essentially allows to load ONNX models. Note that an ONNX model is loaded as an\n    `InferenceModel` and not as a `Model`: many ONNX models come with partial shape and\n    element type information, while tract's `Model` assume full shape and element type\n    knownledge. In this case, it is generally sufficient to inform tract about the input\n    shape and type, then let tract *infer* the rest of the missing shape information\n    before converting the `InferenceModel` to a regular `Model`.\n\n    ```python\n    # load the model as an InferenceModel\n    model = tract.onnx().model_for_path(\"./mobilenetv2-7.onnx\")\n\n    # set the shape and type of its first and only input\n    model.set_input_fact(0, \"1,3,224,224,f32\")\n\n    # get ready to run the model\n    model = model.into_optimized().into_runnable()\n    ```\n    \"\"\"\n\n    def __init__(self):\n        ptr = c_void_p()\n        check(lib.tract_onnx_create(byref(ptr)))\n        self.ptr = ptr\n\n    def __del__(self):\n        check(lib.tract_onnx_destroy(byref(self.ptr)))\n\n    def model_for_path(self, path: Union[str, Path]) -&gt; InferenceModel:\n\"\"\"\n        Load an ONNX file as an InferenceModel\n        \"\"\"\n        model = c_void_p()\n        path = str(path).encode(\"utf-8\")\n        check(lib.tract_onnx_model_for_path(self.ptr, path, byref(model)))\n        return InferenceModel(model)\n</code></pre>"},{"location":"onnx/#tract.onnx.Onnx-functions","title":"Functions","text":""},{"location":"onnx/#tract.onnx.Onnx.model_for_path","title":"<code>model_for_path(path: Union[str, Path]) -&gt; InferenceModel</code>","text":"<p>Load an ONNX file as an InferenceModel</p> Source code in <code>tract/onnx.py</code> <pre><code>def model_for_path(self, path: Union[str, Path]) -&gt; InferenceModel:\n\"\"\"\n    Load an ONNX file as an InferenceModel\n    \"\"\"\n    model = c_void_p()\n    path = str(path).encode(\"utf-8\")\n    check(lib.tract_onnx_model_for_path(self.ptr, path, byref(model)))\n    return InferenceModel(model)\n</code></pre>"},{"location":"runnable/","title":"Runnable model","text":""},{"location":"value/","title":"Value","text":""}]}